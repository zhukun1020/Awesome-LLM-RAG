# Enhancing Large Language Models with Retrieval-Augmented Techniques: Trends, Challenges, and Prospects

This repo aims to record advanced papers of Retrieval-Augmented Generation (RAG) in LLMs. 

## When and What to Retrieve

  1.  [Unified Active Retrieval for Retrieval Augmented Generation](http://arxiv.org/abs/2406.12534 )
 2.  [Active Retrieval Augmented Generation](https://arxiv.org/abs/2305.06983)
 3.  [DRAGIN: Dynamic retrieval augmented generation based on the real-time information needs of large language models](https://arxiv.org/abs/2403.10081)
 4.  [Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions](https://arxiv.org/pdf/2212.10509) 
 5.  [RichRAG: Crafting Rich Responses for Multi-faceted Queries in Retrieval-Augmented Generation](https://arxiv.org/abs/2406.12566)
 6.  [Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity](http://arxiv.org/abs/2403.14403)

## Retrieval Context Processing

 1.  [Benchmarking Large Language Models in Retrieval-Augmented Generation](http://arxiv.org/abs/2309.01431)
 2.  [Bridging the Preference Gap between Retrievers and LLMs](http://arxiv.org/abs/2401.06954 )
 3.  [PRCA: Fitting Black-Box Large Language Models for Retrieval Question Answering via Pluggable Reward-Driven Contextual Adapter ](https://aclanthology.org/2023.emnlp-main.326)
 4.  [RECOMP: IMPROVING RETRIEVAL-AUGMENTED LMS WITH CONTEXT COMPRESSION AND SELECTIVE AUGMENTATION](https://arxiv.org/abs/2310.04408)
 5.  [An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation](https://arxiv.org/abs/2406.01549) 
 6.  [RaFe: Ranking Feedback Improves Query Rewriting for RAG](http://arxiv.org/abs/2405.14431 )
 7.  [RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059)
 8.  [From Local to Global: A Graph RAG Approach to Query-Focused Summarization](https://arxiv.org/pdf/2404.16130)
 9.  [FacTool: Factuality Detection in Generative AI -- A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios](https://arxiv.org/abs/2307.13528)

## Faithful Generation

 1.  [ClashEval: Quantifying the tug-of-war between an LLM's internal prior and external evidence](https://arxiv.org/abs/2404.10198v2)
 2.  [Learning to Plan and Generate Text with Citations](https://arxiv.org/abs/2404.03381)
 3.  [Learning Fine-Grained Grounded Citations for Attributed Large Language Models](https://openreview.net/forum?id=7atXKldh-r)
 4.  [Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection](http://arxiv.org/abs/2310.11511)
 5.  [Model Internals-based Answer Attribution for Trustworthy Retrieval-Augmented Generation](https://arxiv.org/abs/2406.13663)
 6.  [Trusting Your Evidence: Hallucinate Less with Context-aware Decoding](https://aclanthology.org/2024.naacl-short.69/)
 7.  [Synchronous Faithfulness Monitoring for Trustworthy Retrieval-Augmented Generation](https://arxiv.org/abs/2406.13692)

## Future

 1.  [RankRAG: Unifying Context Ranking with Retrieval-Augmented Generation in LLMs](https://arxiv.org/abs/2407.02485v1)

 2.  [Mitigating the Privacy Issues in Retrieval-Augmented Generation (RAG) via Pure Synthetic Data](https://arxiv.org/pdf/2406.14773)

 3.  [TrojanRAG: Retrieval-Augmented Generation Can Be Backdoor Driver in Large Language Models](https://arxiv.org/abs/2405.13401)

 4.  [Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?](https://arxiv.org/abs/2401.11911)

 5.  [Spiral of Silence: How is Large Language Model Killing Information Retrieval?—A Case Study on Open Domain Question Answering](https://arxiv.org/pdf/2404.10496 )

 6.  [BRIGHT: A Realistic and Challenging Benchmark for Reasoning-Intensive Retrieval](https://arxiv.org/abs/2407.12883)

 7.  [Mem0: The Memory Layer for Personalized AI ration](https://github.com/mem0ai/mem0)

     